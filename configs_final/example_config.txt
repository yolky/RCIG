This is an explanation for what each of the hyperparameters does

{
    "has_bn": false, #whether the model used during training uses batchnorm
    "pool_model_count": 30, #number of models to have in the pool
    "l2_rate": 0.0005, #the l2 parameter for the inner problem is l2_rate * coreset_size
    "n_inner_steps": 20, #number of inner optimization steps
    "max_online_steps": 100, #maximum trianing iteration for models in the pool
    "linearize": true, #whether to use linearization
    "aug": null, #What augmentation to use during distillation (in the paper we did not use this, so to replicate set to null)
    "test_aug": "flip_color_crop_rotate_translate_cutout", #What data augmentation to use during testing
    "n_max_steps_pool": 16, #max number of training steps for online models. We do unif(0, n_max_steps_pool) training steps for the pool models. Note that this is different than FRePo which always does 1 steps. We do this because our algorithm is generally slower than FRePo, but we run for fewer training iterations
    "n_hinv_steps": 20, #number of steps used when computing hessian inverse vector products
    "learn_labels": true, #whether to learn the labels
    "monitor_losses": false, #whether to print out the inner/hessian inverse losses (for debugging purposes)
    "pool_learning_rate": 0.00005, #learning rate for the online/pool models
    "proto_learning_rate": 0.003, #learning rate for the coreset images
    "img_size": 32, #resolution of the image dataset
    "aug_repeats": 0, #just set this to 0 - we didn't use this in the paper
    "normal_repeats": 1, #Set this to 1 if you are not using the subsampling trick for larger datasets. Otherwise, this corresponds to the number of partial backwards passes to use
    "softplus_temp": 60, #For depth 3 models we replace the ReLu with a softplus with a temperature (we use standard relus during testing). For depth 4/5, we use standard relus (set softplus_temp = 0)
    "direct_batch_sizes": [ #The next few batch sizes are related to the subsampling technique we use for larger coreset sizes. In general, set these to null if you have enough GPU memory
        null, #This is the number of samples in the training set to propagate gradients through when compute the direct gradient
        null #This is the number of samples in the distilled dataset to propagate gradients through when compute the direct gradient
    ],
    "implicit_batch_size": null, #This is the number of samples in the distilled dataset to propagate gradients through when compute the the implicit gradient
    "hinv_batch_size": null, #This is the number of samples in the distilled dataset to propagate gradients through when compute the the hessian inverse vector product
    "inner_train_batch_size": null, #This is the number of samples in the distilled dataset to propagate gradients through when optimizing the inner problem
    "pool_train_batch_size": null, #This is the number of samples in the distilled dataset to propagate gradients through when training the pool models
    "max_forward_batch_size": null, #This is maximum batch size for models which we do not backpropagate through (i.e. only doing forward passes through)
    "do_precompute": false, #Set this to true if any of the previous batch sizes are not null - it determines whether we use the subsampling trick or not
    "use_flip": true, #Whether we use the flip trick proposed in FRePo. Set this to true for all RGB datasets, if you have enough VRAM
    "checkpoint_iters": [50, 100, 200, 300, 400, 500, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 15000] #what iterations to save checkpoints at
}


For MNIST, and Fashion-MNIST, we use the config depth_3_no_flip.txt
For CIFAR-10, CIFAR-100, and CUB-200, we use config depth_3.txt
For CIFAR-100 with 50 ipc we use config cifar100_50.txt
For T-ImageNet 1 ipc, we use config depth_4_200.txt
For T-ImageNet 10 ipc and resized Imagenet, we use config depth_4_big.txt
For ImageNette and ImageWoof, we use config depth_5.txt

For the number of training iterations (i.e. the --max_steps argument, see section S.4.2 in the appendix)